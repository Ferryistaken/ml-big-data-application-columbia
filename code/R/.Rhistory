)
# fit model with our training data set, training will be done for 200 times data set
fit = model %>%
fit(
x = x,
y = y,
shuffle = T,
batch_size = 5,
validation_split = 0.3,
epochs = 200
)
plot_model(model)
k = backend()
sess = k$get_session()
sess = k$backend()
sess$list_devices()
tf$config$experimental$list_physical_devices()
library(tensorflow)
tf$config$experimental$list_physical_devices()
rm(list = ls())
gc()
library(keras)
library(tensorflow)
library(dygraphs)
use_session_with_seed(1,disable_gpu = FALSE)
data = iris[sample(nrow(iris)),]
y = data[, "Species"]
x = data[,1:4]
# scale to [0,1]
x = as.matrix(apply(x, 2, function(x) (x-min(x))/(max(x) - min(x))))
# one hot encode classes / create DummyFeatures
levels(y) = 1:length(y)
y = to_categorical(as.integer(y) - 1 , num_classes = 3)
# create sequential model
model = keras_model_sequential()
# add layers, first layer needs input dimension
model %>%
layer_dense(input_shape = ncol(x), units = 10, activation = "relu") %>%
layer_dense(units = 10, activation = "relu") %>%
layer_dense(units = 3, activation = "softmax")
# add a loss function and optimizer
model %>%
compile(
loss = "categorical_crossentropy",
optimizer = "adagrad",
metrics = "accuracy"
)
tf$config$experimental$list_physical_devices()
# fit model with our training data set, training will be done for 200 times data set
fit = model %>%
fit(
x = x,
y = y,
shuffle = T,
batch_size = 5,
validation_split = 0.3,
epochs = 200
)
plot_model(model)
install_keras
install_keras(tensorflow = "gpu")
install_keras(tensorflow = "default")
install_keras(tensorflow = "gpu")
require(devtools)
install_version("keras", version = "2.3.0.0")
library(keras)
install_keras(tensorflow = "gpu")
library(keras)
install_keras(tensorflow = "gpu")
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
library(keras)
set_random_seed(1, disable_gpu = FALSE)
library(keras)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
library(keras)
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(28, 28)) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(10, activation = "softmax")
summary(model)
model %>%
compile(
loss = "sparse_categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
model %>%
fit(
x = mnist$train$x, y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 2
)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
rm(list = ls())
gc()
library(keras)
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(28, 28)) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(10, activation = "softmax")
summary(model)
model %>%
compile(
loss = "sparse_categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
model %>%
fit(
x = mnist$train$x, y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 2
)
rm(list = ls())
gc()
library(keras)
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(28, 28)) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(10, activation = "softmax")
summary(model)
model %>%
compile(
loss = "sparse_categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
model %>%
fit(
x = mnist$train$x, y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 2
)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
model %>%
fit(
x = mnist$train$x, y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 1
)
history = model %>%
fit(
x = mnist$train$x, y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 1
) plot(history)
history = model %>%
fit(
x = mnist$train$x, y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 1
); plot(history)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
rm(list = ls())
gc()
library(keras)
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(28, 28)) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(10, activation = "softmax")
model %>%
compile(
loss = "sparse_categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
history = model %>%
fit(
x = mnist$train$x, y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 1
); plot(history, type = "b")
rm(list = ls())
gc()
Sys.setenv("CUDA_VISIBLE_DEVICES" = -1)
library(keras)
use_backend("tensorflow")
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(28, 28)) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(10, activation = "softmax")
summary(model)
model %>%
compile(
loss = "sparse_categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
history = model %>%
fit(
x = mnist$train$x, y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 1
); plot(history, type = "b")
rm(list = ls())
gc()
Sys.setenv("CUDA_VISIBLE_DEVICES" = -1)
rm(list = ls())
gc()
Sys.setenv("CUDA_VISIBLE_DEVICES" = -1)
library(keras)
use_backend("tensorflow")
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(28, 28)) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(10, activation = "softmax")
summary(model)
model %>%
compile(
loss = "sparse_categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
history = model %>%
fit(
x = mnist$train$x, y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 1
); plot(history, type = "b")
history = model %>%
fit(
x = mnist$train$x, y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 1
); plot(history, type = "b")
history = model %>%
fit(
x = mnist$train$x, y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 1
); plot(history, type = "b")
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
Sys.setenv("CUDA_VISIBLE_DEVICES" = 1)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
compile
rm(list = ls())
gc()
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(28, 28)) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(10, activation = "softmax")
model %>%
compile(
loss = "sparse_categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
history = model %>%
fit(
x = mnist$train$x, y = mnist$train$y,
epochs = 10,
validation_split = 0.3,
verbose = 1
); plot(history, type = "b")
View(mnist)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
rm(list = ls())
gc()
read.csv("~/2021-07-07-AccountStatement.csv")
rm(list = ls())
gc()
library(keras)
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(28, 28)) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(10, activation = "softmax")
summary(model)
model %>%
compile(
loss = "sparse_categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
history = model %>%
fit(
x = mnist$train$x,
y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 1
); plot(history, type = "b")
rm(list = ls())
gc()
rm(list = ls(all = TRUE))
gc()
library(keras)
library(keras)
mnist <- dataset_mnist()
mnist$train$x <- mnist$train$x/255
mnist$test$x <- mnist$test$x/255
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(28, 28)) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(10, activation = "softmax")
summary(model)
model %>%
compile(
loss = "sparse_categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy"
)
history = model %>%
fit(
x = mnist$train$x,
y = mnist$train$y,
epochs = 5,
validation_split = 0.3,
verbose = 1
); plot(history, type = "b")
plot(history, type = "b")
plot(history, type = "l")
history
plot(history)
plot(history, col = "2")
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
rm(list = ls(all = TRUE))
gc()
model <- keras_model_sequential() %>%
layer_flatten(input_shape = c(28, 28)) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(0.2) %>%
layer_dense(10, activation = "softmax")
gc()
?install_keras
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
model %>% evaluate(mnist$test$x, mnist$test$y)
model %>% predict_classes(mnist$test$x)
model.predict(mnist$test$x)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day6/neural-network.R", echo=TRUE)
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("EBImage")
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("EBImage")
install.packages("jpeg")
install.packages("jpeg")
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("EBImage")
rm(list = ls())
source("~/.active-rstudio-document", echo=TRUE)
install.packages("TTR")
library(TTR)
SMA(MSFT, n = 50)
SMA(MSFT$MSFT.Close, n = 50)
ma = SMA(MSFT$MSFT.Close, n = 50)
plot(MSFT$MSFT.Close)
lines()
lines(ma)
plot(MSFT$MSFT.Close, col = "green")
lines(ma, col = "blue")
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/moving-average.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/moving-average.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/moving-average.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/moving-average.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/moving-average.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/moving-average.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/moving-average.R", echo=TRUE)
dygraph(SPY$SPY.Close, col = "green")
library(dygraphs)
dygraph(SPY$SPY.Close, col = "green")
dygraph(SPY$SPY.Close)
lines(ma50, col = "blue")
rm(list = ls())
gc()
library(quantmod)
library(TTR)
library(dygraphs)
getSymbols("SPY",
from  = "2015-01-01")
ma50 = SMA(SPY$SPY.Close, n = 50)
ma200 = SMA(SPY$SPY.Close, n = 200)
dygraph(SPY$SPY.Close)
lines(ma50, col = "blue")
lines(ma200, col = "red")
plot(SPY$SPY.Close)
lines(ma50, col = "blue")
lines(ma200, col = "red")
View(ma50)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/general-code/monthly-growth-strategy/growth-strategy.R", echo=TRUE)
rm(list = ls())
gc()
imdb <- dataset_imdb(num_words = max_features)
library(keras)
max_features <- 20000
imdb <- dataset_imdb(num_words = max_features)
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
dictionary = keras::dataset_imdb_word_index()
dictionary = data.frame(unlist(dictionary))
dictionary = data.frame(name = rownames(dictionary), idx = dictionary$unlist.dictionary.)
View(dictionary)
dictionary[x_train[1, ], 1]
dictionary = keras::dataset_imdb_word_index()
dictionary = data.frame(unlist(dictionary))
dictionary = data.frame(name = rownames(dictionary), idx = dictionary$unlist.dictionary.)
dictionary = dictionary[order(dictionary$idx), ]
dictionary[x_train[1, ], ]
dictionary[x_train[1, ], 1]
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day8/recurring-neural-network.R", echo=TRUE)
dictionary[x_train[1, ], 1]
dictionary[x_train[1,], ]
dictionary[x_train[1,], 1]
dictionary[x_train[1,], ]
dictionary[x_train[1,], 1]
x_train[1,]
x_train <- imdb$train$x
x_train[1,]
imdb <- dataset_imdb(num_words = max_features)
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
x_train[1,]
x_train
View(x_test)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day8/recurring-neural-network.R", echo=TRUE)
x_train
dictionary[x_train[1,], ]
dim(x_train)
x_train <- imdb$train$x
y_train <- imdb$train$y
x_test <- imdb$test$x
y_test <- imdb$test$y
dim(x_train)
View(x_train)
View(x_train)
x_train <- imdb$train$x
dim(x_train)
dim(imdb$train$x)
x_train <- pad_sequences(x_train, maxlen = maxlen)
View(x_train)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day8/recurring-neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day8/recurring-neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day8/recurring-neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day8/recurring-neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day8/recurring-neural-network.R", echo=TRUE)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day8/recurring-neural-network.R", echo=TRUE)
scores <- model %>% evaluate(
x_test, y_test,
batch_size = batch_size
)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day8/recurring-neural-network.R", echo=TRUE)
model %>% save_model_tf("trained_model")
fakeData = data.frame(
colA = rep(c("A", "B", "C"), 4),
colB = rnorm(12),
colC = c(rnorm(6, mean = 0, sd = 1), rnorm(6, mean = 10, sd = 1))
)
View(fakeData)
library(dygraphs)
dygraph(fakeData)
plot(fakeData)
source("~/Documents/learning/columbia/columbia-machine-learning-big-data-real-world-applications/code/R/day9/feature-engineering.R", echo=TRUE)
dev.off()
install.packages("lintr")
